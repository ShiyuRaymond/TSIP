import torch
import torch.nn as nn
import torch.nn.functional as F


import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger

from transformers import AutoModel,AutoTokenizer, AutoModelForCausalLM, AutoConfig  
from transformers.models.opt import OPTForCausalLM
from transformers import BartTokenizer, BartForConditionalGeneration
from transformers.modeling_outputs import BaseModelOutput

from transformers import BertForMaskedLM

from transformers import get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup


from MSVD_MODE.video_caption import YOLOWorldCaptioning
from utils.nlp_metrics.NLP_metrics import  nlp_metric_bert
from utils.track_cuda import track_cuda_memory
from utils.loss import compute_mlm_loss
import pickle 
from omegaconf import OmegaConf



def set_requires_grad(module, flag: bool):
    """é€’å½’ä¿®æ”¹ä¸€ä¸ª module é‡Œçš„æ‰€æœ‰å‚æ•°çš„ requires_grad"""
    for p in module.parameters():
        p.requires_grad = flag


def unfreeze_llm_last_n_layers(llm, n_last: int = 2):
    """
    ä»…è§£å†» decoder çš„æœ€å n_last å±‚ã€‚
    é€‚é…å¸¸è§ HuggingFace decoder-only æˆ– encoder-decoder æ¨¡å‹ã€‚
    """
    total_layers = llm.config.num_hidden_layers
    target_ids = {total_layers - i - 1 for i in range(n_last)}  # æœ« N å±‚ idx
    for name, p in llm.named_parameters():
        if any(f"layers.{idx}." in name for idx in target_ids):
            p.requires_grad = True

def strip_prefix(pred: str, prefix="a video of "):
    if pred.lower().startswith(prefix):
        return pred[len(prefix):].strip()
    return pred



class TSIP(pl.LightningModule):
    def __init__(self, 
                 cfg,
                #  stage: int = 1, 
                 lr: float = 1e-4, 
                 dropout: float = 0.1, 
                 grad_clip: float = 1.0,
                #  temperature = 0.07,
                #  embed_dim = 512,
                #  llm_name = 'bert-base-uncased',
                #  max_length = 50,
                 ):
        
        """
        stage: 1 è¡¨ç¤ºè½¯æç¤ºåŒ¹é…è®­ç»ƒé˜¶æ®µï¼Œ2 è¡¨ç¤ºæè¿°ç”Ÿæˆé˜¶æ®µã€‚
        lr: å­¦ä¹ ç‡ã€‚
        dropout: æ¨¡å‹ä¸­ä½¿ç”¨çš„ dropout æ¦‚ç‡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ã€‚
        grad_clip: æ¢¯åº¦è£å‰ªå€¼ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ã€‚
        """
        super().__init__()
        self.save_hyperparameters()
        self.model = YOLOWorldCaptioning(cfg.base_model)  # å®ä¾‹åŒ–åº•å±‚æ¨¡å‹
        self.stage = cfg.stage
        self.max_length = cfg.model.max_length
                      
        # ç¬¬äºŒé˜¶æ®µï¼Œé»˜è®¤è§£å†»éœ€è¦å¾®è°ƒçš„æ¨¡å—, ä½¿ç”¨llmè¿›è¡Œè®­ç»ƒ
        self.accuracy = nlp_metric_bert()
        
        
        # åŠ è½½LLMå’ŒTokenizer
        llm_config = AutoConfig.from_pretrained("facebook/bart-base")

        self.llm = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
        # self.tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
        
        self.unfreeze_llm_last_n = 2
        self._configure_freeze_policy(self.unfreeze_llm_last_n)
        
    def _configure_freeze_policy(self, unfreeze_llm_last_n=2):
        """
        ç»Ÿä¸€å†»ç»“ç­–ç•¥ï¼š
        â€¢ å…¨éƒ¨ freeze
        â€¢ æŒ‡å®šæ¨¡å—è§£å†»
        â€¢ å¯é€‰ï¼šLLM å N å±‚è§£å†»
        """
        # 1) å†»ç»“å…¨æ¨¡å‹
        set_requires_grad(self, False)

        # 2) å¿…é¡»è§£å†»çš„è§†è§‰â†’LLMæ¡¥æ¥
        

        # 3) è‹¥éœ€è¦è®­ç»ƒ early fusion / projectorï¼Œå¯åœ¨æ­¤è§£å†»
        #    ç¤ºèŒƒï¼šä¿æŒå†»ç»“ -> ä¸åšæ“ä½œ
        set_requires_grad(self.model.vis_proj, True)
        set_requires_grad(self.model.fusion_module, True)
        # set_requires_grad(self.model.scale_attention, True)
        # set_requires_grad(self.model.asymmetric_fusion_module, True)
        set_requires_grad(self.model.detector.neck.text_enhancer, True)
        set_requires_grad(self.model.projector, True)
        # # 3) è§£å†» LLM çš„ decoder æœ€å2å±‚
        total_dec_layers = self.llm.config.decoder_layers
        dec_target_ids = {total_dec_layers - i - 1 for i in range( unfreeze_llm_last_n )}
        for name, param in self.llm.model.decoder.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                param.requires_grad = True

        # 4) è§£å†» LLM çš„ encoder æœ€å2å±‚
        # total_enc_layers = self.llm.config.encoder_layers
        # enc_target_ids = {total_enc_layers - i - 1 for i in range(2)}
        # for name, param in self.llm.model.encoder.named_parameters():
        #     if any(f"layers.{i}." in name for i in enc_target_ids):
        #         param.requires_grad = True

        # # 5) è§£å†» lm_headï¼ˆç”¨äº MLMï¼‰
        # set_requires_grad(self.llm.lm_head, True)
        # =====================================================

    def setup(self, stage=None):
        if not hasattr(self, "tokenizer"):
            self.tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")

    @torch.no_grad()
    def generate_caption_from_cross_attn(
        self,
        soft_prompt: torch.Tensor,          # [B, L, D] æ¥è‡ª QFormer è¾“å‡º
        prompt_text: str = "a video of",    # decoder çš„èµ·å§‹æç¤º
        max_new_tokens: int = 32,
        min_length = 6,
        num_beams: int = 5,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        no_repeat_ngram_size=3,
        
    ):
        """
        ä½¿ç”¨ encoder-decoder æ¶æ„ï¼ˆBARTï¼‰+ soft promptï¼ˆQFormerè¾“å‡ºï¼‰ç”Ÿæˆ captionã€‚
        """
        B = soft_prompt.size(0)
        device = soft_prompt.device

        # 1. å‡†å¤‡ decoder èµ·å§‹è¾“å…¥ï¼ˆå¦‚ "a video of"ï¼‰
        decoder_input_ids = self.tokenizer(
            [prompt_text] * B,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True
        ).input_ids.to(device)  # [B, T]

        # 2. æ„é€  encoder attention maskï¼ˆsoft_prompt æ˜¯ encoder_outputsï¼‰
        # encoder_attention_mask = torch.ones(soft_prompt.shape[:-1], dtype=torch.long).to(device)  # [B, L]

        # 3. æ„é€  encoder_outputsï¼ˆæ¨¡æ‹Ÿ Encoder çš„è¾“å‡ºç»“æ„ï¼‰
        encoder_outputs = BaseModelOutput(last_hidden_state=soft_prompt)

        # 4. è°ƒç”¨ generateï¼ˆCross-Attn æ³¨å…¥ soft promptï¼‰
        generated_ids = self.llm.generate(
            input_ids=decoder_input_ids,                         # decoder çš„åˆå§‹ token
            encoder_outputs=encoder_outputs,                     # ğŸ‘ˆ è§†è§‰ soft prompt æ³¨å…¥ç‚¹
            # encoder_attention_mask=encoder_attention_mask,       # å¯¹åº”ä½ç½®çš„ mask
            max_new_tokens=max_new_tokens,
            min_length = min_length,
            num_beams=num_beams,
            do_sample=do_sample,
            temperature=temperature,
            no_repeat_ngram_size = no_repeat_ngram_size,
            # top_k=top_k,
            # top_p=top_p,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            length_penalty=1.2,
            repetition_penalty=1.1
            
        )

        # 5. è§£ç ç”Ÿæˆç»“æœ
        captions = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True
        )

        return captions


    
    def on_fit_start(self):
        if self.stage == 2 :
            self.accuracy.to(self.device)

    def forward(self, frames, video_embedings):
        """å°†è°ƒç”¨åº•å±‚æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€‚"""
        fused_embeddings, soft_prompt = self.model(frames, video_embedings)     # [B, embed_dim]     
        return fused_embeddings, soft_prompt


    def training_step(self, batch, batch_idx):
        frames, captions, video_embedings = batch
        fused_emd, soft_prompt = self(frames, video_embedings)  # åªç”¨å›¾åƒï¼Œæ–‡æœ¬ç­‰ä¸‹å¤„ç†
        
        noise_std = 0.01
        # æ˜¾å¼ detachï¼Œé¿å…å¤šæ¬¡ä½¿ç”¨é€ æˆ autograd graph å †ç§¯
        fused_feature_1 = fused_emd + torch.randn_like(fused_emd) * noise_std
        fused_feature_2 = fused_emd + torch.randn_like(fused_emd) * noise_std
        soft_prompt_1 = self.model.vis_proj(fused_feature_1)
        soft_prompt_2 = self.model.vis_proj(fused_feature_2)
        infor_loss = simclr_infonce_loss(soft_prompt_1, soft_prompt_2)
        infor_loss_val = infor_loss.item()

        batch_size = frames.shape[0]
        
        """è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥ã€‚æ ¹æ®é˜¶æ®µè®¡ç®—ä¸åŒçš„æŸå¤±ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚"""
        # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—ç”Ÿæˆæ–‡æœ¬ä¸çœŸå®æ–‡æœ¬çš„äº¤å‰ç†µæŸå¤±
        torch.cuda.reset_peak_memory_stats()  # ğŸ’¡ é‡ç½®æ˜¾å­˜å³°å€¼ç»Ÿè®¡

        # 1. æ„é€  hard promptï¼šè‡ªç„¶è¯­è¨€æç¤º
    
        # 2. æ„é€  ground-truth captions
        caption_tokenized = self.tokenizer(captions, padding="max_length", truncation=True, max_length=16, return_tensors="pt")
        
        caption_ids = caption_tokenized.input_ids.to(self.device)
        labels = caption_ids.clone()
        labels[labels == self.tokenizer.pad_token_id] = -100         # pad -> -100
        encoder_outputs = BaseModelOutput(last_hidden_state=soft_prompt)

        
        # ä½¿ç”¨hidden stateç­–ç•¥
        outputs = self.llm(
            input_ids=caption_ids,
            labels=labels,
            encoder_outputs=encoder_outputs
            # encoder_attention_mask=encoder_attention_mask
        )
        caption_loss = outputs.loss  
        caption_loss_val = caption_loss.item()
        # mlm_weight = 0.1
        # if self.current_epoch < 2:
        #     mlm_loss = compute_mlm_loss(llm=self.llm,tokenizer=self.tokenizer,device=self.device,captions=captions)
        #     mlm_loss_val = mlm_loss.item()
        # else:
        #     mlm_loss_val = 0
         # ä½¿ç”¨éš epoch è¡°å‡çš„ alpha
        # alpha = compute_infor_weight(self.current_epoch, self.trainer.max_epochs, initial_alpha=1.0, final_alpha=0.0)
        # loss = caption_loss + alpha * infor_loss
        
        loss = caption_loss +  0.2 * infor_loss 
       
        total_loss_val = loss.item()
        
        # 5. è®¡ç®— loss

        # è®°å½•è®­ç»ƒæŸå¤±
        self.log("train/infor_loss", infor_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/caption_loss", caption_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        # self.log("train/mlm_loss", mlm_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/train_loss", total_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        
        

        # 5. æ˜¾å­˜ç»Ÿè®¡æ—¥å¿—
        peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
        self.log("train/peak_mem_MB", peak_mem, prog_bar=True, on_epoch=True)
        # Step 5: æ˜¾å¼é‡Šæ”¾é˜²æ³„æ¼
        del outputs, soft_prompt, caption_ids, labels, caption_loss, infor_loss
        torch.cuda.empty_cache()
        
        
        return loss
   
   
    @torch.no_grad()
    def validation_step(self, batch, batch_idx):
        """éªŒè¯è¿‡ç¨‹ï¼šç”Ÿæˆæè¿°å¹¶æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡ã€‚"""
        frames_tensor, caption, video_embedding = batch
        # 1. ç”Ÿæˆ captionï¼ˆé¢„æµ‹ï¼‰
        fused_emd,soft_prompts = self(frames_tensor,video_embedding)
        
        # tem_res = log_per_prompt_statistics(soft_prompts)
        
        # pask key val ç­–ç•¥ 1
        generated_captions = self.generate_caption_from_cross_attn(
            soft_prompt=soft_prompts
        )
        
        generated_captions = [strip_prefix(p) for p in generated_captions]
        # 2. æ›´æ–° NLP-metricsï¼ˆBLEU, METEOR, CIDEr ç­‰ï¼‰
        self.accuracy.update(generated_captions, caption)
        
        if batch_idx == 0 and self.trainer.is_global_zero:
            for i, (pred, ref) in enumerate(zip(generated_captions, caption)):
                if i >= 2:  # æœ€å¤šè®°å½• 2 ä¸ªæ ·æœ¬ï¼Œé˜²æ­¢æ—¥å¿—è¿‡å¤š
                    break
                self.logger.experiment.add_text(
                    tag=f"val_caption_epoch{self.current_epoch}_sample{i}",
                    text_string=f"**GT:** {ref[:10]}\n**Pred:** {pred}",
                    global_step=self.global_step
                )
        
        del fused_emd, soft_prompts, generated_captions
        torch.cuda.empty_cache()
    
    @torch.no_grad()
    def on_validation_epoch_end(self):
        score_dict = self.accuracy.compute()  # self.val æ˜¯éªŒè¯é›†çš„ reference caption æ•°æ®

        # åˆ†åˆ«è®°å½•å„é¡¹æŒ‡æ ‡
        self.log('val/harmonic_mean_bleu_meteor', self.accuracy.harmonice, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu1', self.accuracy.bleu1, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu2', self.accuracy.bleu2, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu3', self.accuracy.bleu3, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu4', self.accuracy.bleu4, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/meteor', self.accuracy.meteor, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/cider', self.accuracy.cider, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/rougel', self.accuracy.rougel, on_epoch=True, prog_bar=True, logger=True)
        peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
        self.log("val/peak_mem_MB", peak_mem, prog_bar=True, on_epoch=True)

        # é‡ç½®æŒ‡æ ‡ç¼“å­˜
        self.accuracy.reset()

    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•è¿‡ç¨‹ï¼šç”Ÿæˆæè¿°å¹¶æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡ã€‚"""
        frames_tensor, caption, video_embedding = batch
        # 1. ç”Ÿæˆ captionï¼ˆé¢„æµ‹ï¼‰
        fused_emd,soft_prompts = self(frames_tensor,video_embedding)        
        generated_captions = self.generate_caption_from_cross_attn(
            soft_prompt=soft_prompts
        )
        
        generated_captions = [strip_prefix(p) for p in generated_captions]
        # 2. æ›´æ–° NLP-metricsï¼ˆBLEU, METEOR, CIDEr ç­‰ï¼‰
        self.accuracy.update(generated_captions, caption)
        
        if batch_idx == 0 and self.trainer.is_global_zero:
            for i, (pred, ref) in enumerate(zip(generated_captions, caption)):
                if i >= 2:  # æœ€å¤šè®°å½• 2 ä¸ªæ ·æœ¬ï¼Œé˜²æ­¢æ—¥å¿—è¿‡å¤š
                    break
                self.logger.experiment.add_text(
                    tag=f"val_caption_epoch{self.current_epoch}_sample{i}",
                    text_string=f"**GT:** {ref}\n**Pred:** {pred}",
                    global_step=self.global_step
                )
        
        del fused_emd, soft_prompts, generated_captions
        torch.cuda.empty_cache()
    
    
    def on_test_epoch_end(self):
        # 1. è®¡ç®—æ‰€æœ‰ NLP æŒ‡æ ‡
        score_dict = self.accuracy.compute()

        # 2. è®°å½•æ¯ä¸ªæŒ‡æ ‡
        # self.log('test/harmonic_mean_bleu_meteor', self.accuracy.harmonice, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/bleu1', self.accuracy.bleu1, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/bleu2', self.accuracy.bleu2, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/bleu3', self.accuracy.bleu3, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/bleu4', self.accuracy.bleu4, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/meteor', self.accuracy.meteor, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/cider', self.accuracy.cider, on_epoch=True, prog_bar=True, logger=True)
        # self.log('test/rougel', self.accuracy.rougel, on_epoch=True, prog_bar=True, logger=True)


        self.log('test/harmonic_mean_bleu_meteor', torch.tensor(self.accuracy.harmonice,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu1', torch.tensor(self.accuracy.bleu1,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu2', torch.tensor(self.accuracy.bleu2,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu3', torch.tensor(self.accuracy.bleu3,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu4', torch.tensor(self.accuracy.bleu4,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/meteor', torch.tensor(self.accuracy.meteor,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/cider', torch.tensor(self.accuracy.cider,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/rougel', torch.tensor(self.accuracy.rougel,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)

        # å¯é€‰ï¼šè®°å½•æ±‡æ€»æŒ‡æ ‡
        # self.log("test/harmonic_mean_bleu_meteor", score_dict["harmonic_mean"], prog_bar=True, logger=True)

        # 3. æ¸…ç©ºç¼“å­˜
        self.accuracy.reset()

    
    def configure_optimizers(self):
        """æ ¹æ®é˜¶æ®µè¿”å›ç›¸åº”çš„ä¼˜åŒ–å™¨ï¼ˆå’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰ã€‚"""
            
        # 1. æ”¶é›†å„ç»„å‚æ•°
        vis_proj_params     = list(self.model.vis_proj.parameters())
        fusion_params = list( self.model.fusion_module.parameters())
        # asy_params = list(self.model.asymmetric_fusion_module.parameters())
        text_enhancer_params = list(self.model.detector.neck.text_enhancer.parameters())
        projoctor_params = list(self.model.projector.parameters())
        # scal_params = list(self.model.scale_attention.parameters())
        # 2. è§£å†» LLM çš„ decoder æœ€å2å±‚
        dec_params = []
        total_dec_layers = self.llm.config.decoder_layers
        dec_target_ids = {total_dec_layers - i - 1 for i in range( self.unfreeze_llm_last_n )}
        for name, param in self.llm.model.decoder.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                dec_params.append(param)

        # 3. å®šä¹‰ä¼˜åŒ–å™¨ï¼Œåˆ†åˆ«ç»™ä¸åŒç»„ä¸åŒ lr
        optimizer = torch.optim.AdamW([
            {"params": vis_proj_params, "lr": 5e-5 , "weight_decay": 0.01},
            {"params": projoctor_params, "lr": 7e-5, "weight_decay": 0.01},
            {"params": fusion_params  , "lr": 7e-4, "weight_decay": 0.01},
            {"params": text_enhancer_params, "lr": 5e-4, "weight_decay": 0.01},
            {"params": dec_params, "lr": 9e-6, "weight_decay": 0.01},
          
        ])


        # 4. å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆcosine+warmup ä¸¾ä¾‹ï¼‰
        total_steps = int(self.trainer.estimated_stepping_batches)
        
        scheduler = get_cosine_schedule_with_warmup(
                optimizer,
                num_warmup_steps=int(0.1 * total_steps),  # å‰10%æ­¥æ•°ç”¨äºwarmup
                num_training_steps=int(total_steps),
            )
        
        
        # scheduler = get_polynomial_decay_schedule_with_warmup(
        #     optimizer,
        #     num_warmup_steps=0,  # ä¹Ÿå¯åŠ ä¸€ç‚¹
        #     num_training_steps=total_steps,
        #     power=0.8  # çº¿æ€§ä¸‹é™ï¼›è®¾æˆ2.0æ˜¯å¹³æ–¹ä¸‹é™
        # )
        
        # return {
        #     "optimizer": optimizer,
            
        # }

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",   # æ¯stepæ›´æ–°
                "frequency": 1,
            },
            "monitor": "val_cider",  # è¿™è¡Œå…¶å®å¯ä»¥ä¸åŠ ï¼Œå› ä¸ºæˆ‘ä»¬ç”¨cosine
        }

def compute_infonce_loss_video_level(soft_prompts: torch.Tensor, temperature=0.07):
    """
    soft_prompts: Tensor [B, L, D]ï¼Œæ¯ä¸ªè§†é¢‘ä¸€ä¸ª soft prompt åºåˆ—ã€‚
    è¿”å›ï¼šInfoNCE Lossï¼Œé¼“åŠ±ä¸åŒè§†é¢‘çš„ soft prompt å·®å¼‚ã€‚
    """
    B, L, D = soft_prompts.size()
    
    # åš mean pooling å¾—åˆ° [B, D]
    video_reps = F.normalize(soft_prompts.mean(dim=1), dim=-1)

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
    sim_matrix = torch.matmul(video_reps, video_reps.T) / temperature

    # æ¯ä¸€è¡Œçš„å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä»–ä¸ºè´Ÿæ ·æœ¬
    labels = torch.arange(B, device=soft_prompts.device)
    loss = F.cross_entropy(sim_matrix, labels)

    return loss


def simclr_infonce_loss(soft_prompts_1: torch.Tensor, soft_prompts_2: torch.Tensor, temperature=0.07):
    """
    soft_prompts_1 / soft_prompts_2: [B, L, D]ï¼Œæ˜¯åŒä¸€è§†é¢‘çš„ä¸¤ç§è§†è§‰è§†å›¾ä¸‹ç”Ÿæˆçš„ soft prompt
    """
    B, L, D = soft_prompts_1.shape

    # èšåˆ soft promptï¼ˆå¯ç”¨ mean/poolingï¼‰
    z1 = F.normalize(soft_prompts_1.mean(dim=1), dim=-1)  # [B, D]
    z2 = F.normalize(soft_prompts_2.mean(dim=1), dim=-1)  # [B, D]

    # æ‹¼æ¥ï¼šå…± 2B ä¸ªæ ·æœ¬
    z = torch.cat([z1, z2], dim=0)  # [2B, D]
    sim_matrix = torch.matmul(z, z.T) / temperature  # [2B, 2B]

    # æ„é€  maskï¼Œæ’é™¤è‡ªå·±ï¼ˆä¸åŒ…æ‹¬ä¸»å¯¹è§’çº¿ï¼‰
    mask = ~torch.eye(2 * B, dtype=torch.bool, device=z.device)

    # æ¯ä¸ª i çš„æ­£æ ·æœ¬æ˜¯ i+Bï¼ˆå‰åŠæ®µå’ŒååŠæ®µå¯¹åº”ï¼‰
    pos_indices = (torch.arange(B, device=z.device) + B) % (2 * B)
    all_pos_idx = torch.cat([pos_indices, torch.arange(B, device=z.device)])  # [2B]

    # æ¯ä¸ªä½ç½®çš„æ­£æ ·æœ¬ logit
    logits = sim_matrix[mask].view(2 * B, -1)  # é€è¡Œæ’é™¤è‡ªå·± â†’ [2B, 2B-1]
    pos_logit = torch.exp(sim_matrix[torch.arange(2 * B), all_pos_idx])  # [2B]

    denom = torch.sum(torch.exp(logits), dim=1)  # [2B]
    loss = -torch.log(pos_logit / denom)
    return loss.mean()

def compute_infor_weight(current_epoch, total_epochs, initial_alpha=1.0, final_alpha=0.0):
    """çº¿æ€§è¡°å‡ infor_loss æƒé‡"""
    progress = min(current_epoch / total_epochs, 1.0)
    return initial_alpha * (1 - progress) + final_alpha * progress


def log_per_prompt_statistics(soft_prompts: torch.Tensor, logger=None, prefix="soft_prompt", step=None):
    """
    è®°å½•æ¯ä¸ª soft prompt æ ·æœ¬çš„ mean å’Œ varianceï¼ˆæŒ‰ [L, D] ç»´åº¦è¿›è¡Œç»Ÿè®¡ï¼‰

    å‚æ•°:
    - soft_prompts: Tensor [B, L, D]
    - logger: å¯é€‰ï¼ŒPyTorch Lightning çš„ loggerï¼ˆç”¨äº TensorBoardï¼‰
    - prefix: ç”¨äº log çš„å‰ç¼€å
    - step: global_stepï¼ˆå¯é€‰ï¼Œæ§åˆ¶å†™å…¥æ­¥æ•°ï¼‰
    """
    B, L, D = soft_prompts.shape

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬è‡ªèº«çš„ mean å’Œ var
    mean_per_prompt = soft_prompts.mean(dim=(1, 2))  # [B]
    var_per_prompt = soft_prompts.var(dim=(1, 2))    # [B]

    if logger is not None:
        for i in range(B):
            logger.experiment.add_scalar(f"{prefix}/sample_{i}_mean", mean_per_prompt[i].item(), global_step=step)
            logger.experiment.add_scalar(f"{prefix}/sample_{i}_variance", var_per_prompt[i].item(), global_step=step)

    return {
        "mean_per_prompt": mean_per_prompt,  # Tensor [B]
        "var_per_prompt": var_per_prompt     # Tensor [B]
    }