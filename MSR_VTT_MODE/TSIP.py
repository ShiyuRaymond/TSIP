import torch
import torch.nn as nn
import torch.nn.functional as F


import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger

from transformers import AutoModel,AutoTokenizer, AutoModelForCausalLM, AutoConfig  
from transformers.models.opt import OPTForCausalLM
from transformers import BartTokenizer, BartForConditionalGeneration
from transformers.modeling_outputs import BaseModelOutput

from transformers import BertForMaskedLM

from transformers import get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup


from MSR_VTT_MODEL.video_caption import YOLOWorldCaptioning
from utils.nlp_metrics.NLP_metrics import  nlp_metric_bert

from utils.track_cuda import track_cuda_memory
from utils.loss import  SoftPromptCenterLoss, soft_prompt_cosine_diversity_loss,embedding_align_loss, grammar_error_loss,no_repeat_ngram_loss, ending_ngram_match_loss,compute_mlm_loss, compute_align_loss, compute_svo_loss,scheduled_sampling, simclr_infonce_loss
import pickle 
from omegaconf import OmegaConf
import video_clip.video_clip as video_clip  # Import the video_clip package


from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

import torch
import os
import random
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.rouge.rouge import Rouge

import re 

import language_tool_python

# 1. åˆå§‹åŒ–è‹±æ–‡çº é”™å™¨
tool = language_tool_python.LanguageTool('en-US')

import re

def remove_consecutive_duplicates(text):
    # å»é™¤è¿ç»­é‡å¤çš„å•è¯ï¼Œä¾‹å¦‚ "man man is" -> "man is"
    return re.sub(r'\b(\w+)( \1\b)+', r'\1', text)

def ensure_complete_sentence(text):
    # è‹¥å¥å­ç¼ºå°‘è°“è¯­åŠ¨è¯ï¼Œå¯ä»¥å°è¯•è¡¥å…¨ï¼ˆç®€å•å¯å‘å¼ï¼‰
    verbs = ["is", "are", "was", "were", "has", "have", "does", "do", "can", "will"]
    if not any(verb in text.lower().split() for verb in verbs):
        # ç®€å•æ·»åŠ ä¸€ä¸ªé»˜è®¤è°“è¯­
        return text + " is happening"
    return text

def polish_caption_for_rouge(text, lowercase=True):
    # å»é™¤é¦–å°¾ç©ºæ ¼
    text = text.strip()

    # 1. å»é‡
    text = remove_consecutive_duplicates(text)

    # 2. è¡¥å…¨è°“è¯­ï¼ˆå¯é€‰ï¼‰
    # text = ensure_complete_sentence(text)

    # 3. æ ‡ç‚¹ç»Ÿä¸€ï¼ˆå¥æœ«åŠ å¥å·ï¼‰
    # if not text.endswith(('.', '!', '?')):
    #     text += '.'

    # # 4. å¤§å°å†™å¤„ç†
    # if lowercase:
    #     text = text.lower()
    # else:
    #     text = text[0].upper() + text[1:]

    return text



def set_requires_grad(module, flag: bool):
    """é€’å½’ä¿®æ”¹ä¸€ä¸ª module é‡Œçš„æ‰€æœ‰å‚æ•°çš„ requires_grad"""
    for p in module.parameters():
        p.requires_grad = flag


def unfreeze_llm_last_n_layers(llm, n_last: int = 2):
    """
    ä»…è§£å†» decoder çš„æœ€å n_last å±‚ã€‚
    é€‚é…å¸¸è§ HuggingFace decoder-only æˆ– encoder-decoder æ¨¡å‹ã€‚
    """
    total_layers = llm.config.num_hidden_layers
    target_ids = {total_layers - i - 1 for i in range(n_last)}  # æœ« N å±‚ idx
    for name, p in llm.named_parameters():
        if any(f"layers.{idx}." in name for idx in target_ids):
            p.requires_grad = True

def strip_prefix(pred: str, prefix="This video shows:"):
    if pred.lower().startswith(prefix):
        return pred[len(prefix):].strip()
    return pred



class TSIP(pl.LightningModule):
    def __init__(self, 
                 cfg,
                 lr: float = 1e-4, 
                 dropout: float = 0.1, 
                 grad_clip: float = 1.0,
                 ):
        
        """
        stage: 1 è¡¨ç¤ºè½¯æç¤ºåŒ¹é…è®­ç»ƒé˜¶æ®µï¼Œ2 è¡¨ç¤ºæè¿°ç”Ÿæˆé˜¶æ®µã€‚
        lr: å­¦ä¹ ç‡ã€‚
        dropout: æ¨¡å‹ä¸­ä½¿ç”¨çš„ dropout æ¦‚ç‡ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ã€‚
        grad_clip: æ¢¯åº¦è£å‰ªå€¼ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ã€‚
        """
        super().__init__()
        self.save_hyperparameters()
        self.model = YOLOWorldCaptioning(cfg.base_model)  # å®ä¾‹åŒ–åº•å±‚æ¨¡å‹
        self.stage = cfg.stage
        self.max_length = cfg.model.max_length
        self.eval_config = cfg.model.eval_config

        # ç¬¬äºŒé˜¶æ®µï¼Œé»˜è®¤è§£å†»éœ€è¦å¾®è°ƒçš„æ¨¡å—, ä½¿ç”¨llmè¿›è¡Œè®­ç»ƒ
        self.accuracy = nlp_metric_bert()
        
        
        # åŠ è½½LLMå’ŒTokenizer
        llm_config = AutoConfig.from_pretrained("facebook/bart-base")

        self.llm = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
        self.video_encoder  =  video_clip.load_finetune_model(self.eval_config)
        
        self.gpt2 = GPT2LMHeadModel.from_pretrained("gpt2").eval()
        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

        # self.sentence_bert = SentenceTransformer('paraphrase-mpnet-base-v2')


        # model_name = "prithivida/grammar_error_correcter_v1"
        # model_name = "vennify/t5-base-grammar-correction"

        # self.grammer_tokenizer = AutoTokenizer.from_pretrained(model_name)
        # self.grammer_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).eval()

        
        # self.tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
        
        self.unfreeze_llm_last_n = 2
        self.unfreeze_mae_n = 2
        self.unfreeze_qformer = 2
        self.unfreze_llm_encoder_n = 2
        self._configure_freeze_policy(self.unfreeze_llm_last_n, self.unfreeze_mae_n, self.unfreeze_qformer,self.unfreze_llm_encoder_n)
        self.video_ids = []


        self.alignment = SoftPromptCenterLoss()   # or "mse", "combine"

        self.cider_scorer = Cider()
        self.rouge_scorer = Rouge()




    def compute_cider_reward(self, predictions, references):
        """
        predictions: æ¨¡å‹ç”Ÿæˆçš„captionåˆ—è¡¨, é•¿åº¦ä¸ºbatch_size
        references: åŠ¨æ€é€‰å–çš„ground-truth captionåˆ—è¡¨, é•¿åº¦ä¸ºbatch_size
        è¿”å›: torch.Tensor (batch_size,) CIDErå¾—åˆ†
        """
        gts = {i: [ref] for i, ref in enumerate(references)}   # referenceæ ¼å¼ä¸ºå­—å…¸å½¢å¼
        res = {i: [pred] for i, pred in enumerate(predictions)}  # predictionä¹Ÿä¸ºå­—å…¸å½¢å¼
        cider_score, _ = self.cider_scorer.compute_score(gts, res)
        cider_scores_tensor = torch.tensor(cider_score, device=self.device, dtype=torch.float32)

         # 2. ROUGE åˆ†æ•°ï¼ˆpycocoevalcapçš„rougeæ˜¯F1ï¼Œå¦‚æœä½ åªæƒ³è¦Recallï¼Œå¯ä»¥å†å¤„ç†ï¼‰
        rouge_score, scores = self.rouge_scorer.compute_score(gts, res)  # rouge_score æ˜¯å¹³å‡ï¼Œscores æ˜¯åˆ—è¡¨
        rouge_tensor = torch.tensor(scores, device=self.device, dtype=torch.float32)
       
        return 0.4* cider_scores_tensor + 0.6*rouge_tensor 
    
    def _configure_freeze_policy(self, unfreeze_llm_last_n=2, unfreeze_mae_n=2, unfreeze_qformer = 2, unfreeze_llm_enc_last_n=1):
        """
        ç»Ÿä¸€å†»ç»“ç­–ç•¥ï¼š
        â€¢ å…¨éƒ¨ freeze
        â€¢ æŒ‡å®šæ¨¡å—è§£å†»
        â€¢ å¯é€‰ï¼šLLM å N å±‚è§£å†»
        """
        # 1) å†»ç»“å…¨æ¨¡å‹
        set_requires_grad(self, False)

        # 2) å¿…é¡»è§£å†»çš„è§†è§‰â†’LLMæ¡¥æ¥
        

        # 3) è‹¥éœ€è¦è®­ç»ƒ early fusion / projectorï¼Œå¯åœ¨æ­¤è§£å†»
        #    ç¤ºèŒƒï¼šä¿æŒå†»ç»“ -> ä¸åšæ“ä½œ
        
        set_requires_grad(self.model.vis_proj, True)
        set_requires_grad(self.model.fusion_module, True)
        set_requires_grad(self.model.detector.neck.text_enhancer, True)
        # set_requires_grad(self.model.detector.neck.top_down_layers, True)
        # set_requires_grad(self.model.detector.neck.downsample_layers, True)
        # set_requires_grad(self.model.detector.neck.bottom_up_layers, True)
        set_requires_grad(self.model.projector, True)
        
        # set_requires_grad(self.sentence_bert, False)
        
        set_requires_grad(self.gpt2, False)
        
        set_requires_grad(self.video_encoder.vision_proj, True)
        
        set_requires_grad(self.llm.model.decoder.embed_tokens, True)
        set_requires_grad(self.llm.model.decoder.layernorm_embedding, True)
        set_requires_grad(self.llm.model.decoder.embed_positions, True)

        # self.video_encoder.query_tokens.requires_grad = True
        # # 3) è§£å†» LLM çš„ decoder æœ€å2å±‚
        total_dec_layers = self.llm.config.decoder_layers
        dec_target_ids = {total_dec_layers - i - 1 for i in range( unfreeze_llm_last_n )}
        for name, param in self.llm.model.decoder.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                param.requires_grad = True
                
        total_enc_layers = self.llm.config.encoder_layers
        enc_target_ids = {total_enc_layers - i - 1 for i in range(unfreeze_llm_enc_last_n)}
        for name, param in self.llm.model.encoder.named_parameters():
            if any(f"layers.{i}." in name for i in enc_target_ids):
                param.requires_grad = True

        # è§£å†»æœ€å2å±‚
        total_videoencoder_layers = len(self.video_encoder.video_Qformer.bert.encoder.layer)
        dec_target_ids = {total_videoencoder_layers - i - 1 for i in range(unfreeze_mae_n)}
        for name, param in self.video_encoder.video_Qformer.bert.encoder.layer.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                param.requires_grad = True
                
        total_Qformerencoder_layers = len(self.video_encoder.Qformer.bert.encoder.layer)
        dec_target_ids = {total_Qformerencoder_layers - i - 1 for i in range(unfreeze_qformer)}
        for name, param in self.video_encoder.Qformer.bert.encoder.layer.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                param.requires_grad = True
        
        
        # num_layers = len( self.video_encoder.video_Qformer.bert.encoder.layer)
        # n_unfreeze = unfreeze_mae_n
        # for i in range(num_layers - n_unfreeze, num_layers):
        #     for param in  self.video_encoder.video_Qformer.bert.encoder.layer[i].parameters():
        #         param.requires_grad = True

        # 4) è§£å†» LLM çš„ encoder æœ€å2å±‚
        # total_enc_layers = self.llm.config.encoder_layers
        # enc_target_ids = {total_enc_layers - i - 1 for i in range(2)}
        # for name, param in self.llm.model.encoder.named_parameters():
        #     if any(f"layers.{i}." in name for i in enc_target_ids):
        #         param.requires_grad = True

        # # 5) è§£å†» lm_headï¼ˆç”¨äº MLMï¼‰
        # set_requires_grad(self.llm.lm_head, True)
        # =====================================================

    def setup(self, stage=None):
        if not hasattr(self, "tokenizer"):
            self.tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")

    
    def compute_gpt2_ppl(self, sent_list):
        # sent_list: list of strings
        scores = []
        for sent in sent_list:
            enc = self.gpt2_tokenizer(sent, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.gpt2(**enc, labels=enc["input_ids"])
                loss = outputs.loss.item()
                scores.append(loss)
        return scores  # lossè¶Šä½è¶Šæµç•…
    
    def repetition_penalty_reward(self, caption):
        # æƒ©ç½šé‡å¤ç‰‡æ®µï¼Œä¾‹å¦‚3-gramé‡å¤
        tokens = caption.split()
        seen = set()
        for i in range(len(tokens)-2):
            ngram = tuple(tokens[i:i+3])
            if ngram in seen:
                return 0.0
            seen.add(ngram)
        return 1.0
    
    

    @torch.no_grad()
    def generate_caption_from_cross_attn(
        self,
        soft_prompt: torch.Tensor,          # [B, L, D] æ¥è‡ª QFormer è¾“å‡º
        prompt_text: str = "This video shows:",    # decoder çš„èµ·å§‹æç¤º
        max_new_tokens: int = 64,
        min_length =6,
        num_beams: int = 8,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_k: int = 50,
        # top_p: float = 0.79,
        no_repeat_ngram_size=2,
        repetition_penalty=1.5,
        length_penalty=0.7,
    ):
        """
        ä½¿ç”¨ encoder-decoder æ¶æ„ï¼ˆBARTï¼‰+ soft promptï¼ˆQFormerè¾“å‡ºï¼‰ç”Ÿæˆ captionã€‚
        """
        B = soft_prompt.size(0)
        device = soft_prompt.device

        # 1. å‡†å¤‡ decoder èµ·å§‹è¾“å…¥ï¼ˆå¦‚ "a video of: : "ï¼‰
        decoder_input_ids = self.tokenizer(
            [prompt_text] * B,
            return_tensors="pt",
            padding=True,
            add_special_tokens=True
        ).input_ids.to(device)  # [B, T]

        # 2. æ„é€  encoder attention maskï¼ˆsoft_prompt æ˜¯ encoder_outputsï¼‰
        # encoder_attention_mask = torch.ones(soft_prompt.shape[:-1], dtype=torch.long).to(device)  # [B, L]

        # 3. æ„é€  encoder_outputsï¼ˆæ¨¡æ‹Ÿ Encoder çš„è¾“å‡ºç»“æ„ï¼‰
        encoder_outputs = BaseModelOutput(last_hidden_state=soft_prompt)

        # è·å–promptçš„tokenæ•°ï¼Œç”¨äºä¿®æ­£min_length
        prompt_token_len = decoder_input_ids.shape[1]
        # min_length åº”è®¾ç½®ä¸º prompt+8ï¼Œä¿è¯æ–°ç”Ÿæˆéƒ¨åˆ†â‰¥8
        effective_min_length = prompt_token_len + min_length


        # 4. è°ƒç”¨ generateï¼ˆCross-Attn æ³¨å…¥ soft promptï¼‰
        generated_ids = self.llm.generate(
            input_ids=decoder_input_ids,                         # decoder çš„åˆå§‹ token
            encoder_outputs=encoder_outputs,                     # ğŸ‘ˆ è§†è§‰ soft prompt æ³¨å…¥ç‚¹
            # encoder_attention_mask=encoder_attention_mask,       # å¯¹åº”ä½ç½®çš„ mask
            max_new_tokens=max_new_tokens,
            min_length = min_length,
            num_beams=num_beams,
            do_sample=do_sample,
            temperature=temperature,
            no_repeat_ngram_size = no_repeat_ngram_size,
            top_k=top_k,
            # top_p=top_p,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            length_penalty=length_penalty,
            repetition_penalty=repetition_penalty,
            num_return_sequences=num_beams,
            # num_beam_groups=2,      # åˆ†ç»„æå‡å¤šæ ·æ€§
            # diversity_penalty=0.3
        )
        # 5. è§£ç ç”Ÿæˆç»“æœ
        captions = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True
        )
        candidates = [captions[i:i+num_beams] for i in range(0, len(captions), num_beams)]  

        try:  
            final_caps, _ = zip(*[self.rerank_candidates([c for c in cand_list if c.strip()]) for cand_list in candidates])  # æŒ‰batch

        except:
            print(candidates)
        
        return final_caps
    
    @torch.no_grad()
    def gpt2_score(self, caption):
        # GPT2å¾—åˆ†è¶Šé«˜ï¼Œè¯´æ˜è¶Šæµç•…/naturalï¼ˆé€šå¸¸ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶/perplexityï¼‰
        tokens = self.gpt2_tokenizer.encode(caption, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.gpt2(tokens, labels=tokens)
            loss = outputs.loss.item()
        # æ³¨æ„ï¼Œlossè¶Šå°ï¼Œè¯´æ˜æ¨¡å‹è¶Šè®¤å¯
        return -loss
    
    @torch.no_grad()
    def gpt2_local_ngram_reward(self, caption, n=4):
        """
        caption: str
        n: int, n-gramçª—å£é•¿åº¦ï¼ˆå¦‚4ï¼‰
        è¿”å›: æ‰€æœ‰n-gramçš„gpt2å¾—åˆ†å¹³å‡å€¼
        """
        tokens = caption.strip().split()
        if len(tokens) < n:
            # çŸ­å¥ç›´æ¥ç”¨æ•´ä½“score
            return self.gpt2_score(caption)
        scores = []
        for i in range(len(tokens) - n + 1):
            ngram = " ".join(tokens[i:i+n])
            score = self.gpt2_score(ngram)
            scores.append(score)
        return sum(scores) / len(scores)

    # @torch.no_grad()
    # def grammar_correct(self, sentence, max_new_tokens=64):
    #     # ç›´æ¥è¾“å…¥é”™å¥ï¼Œä¸éœ€è¦åŠ æŒ‡ä»¤prompt
    #     input_ids = self.grammer_tokenizer.encode(sentence, return_tensors='pt').to(self.device)
    #     with torch.no_grad():
    #         outputs = self.grammer_model.generate(
    #             input_ids=input_ids,
    #             max_new_tokens=64,         # é€šå¸¸è¶³å¤Ÿï¼Œé•¿å¥å¯é€‚å½“åŠ å¤§
    #             num_beams=4,               # 4-5ä¸ªbeamæ¯”1æ›´é²æ£’ï¼Œæ˜“å‡ºæœ€ä¼˜ç»“æœï¼Œå‡ ä¹ä¸å½±å“é€Ÿåº¦
    #             do_sample=False,           # ä¸é‡‡æ ·ï¼Œç»“æœæ›´ç¨³å®š
    #             early_stopping=True,       # ä¼˜å…ˆåœæ­¢
    #             use_cache=False,           # å¿…é¡»åŠ ï¼Œé¿å…cacheç›¸å…³bug
    #             no_repeat_ngram_size=3,    # é˜²æ­¢é‡å¤ï¼Œæå‡å¥å­è´¨é‡
    #             repetition_penalty=1.1,    # ç¨å¾®æƒ©ç½šé‡å¤
    #         )
    #     result = self.grammer_tokenizer.decode(outputs[0], skip_special_tokens=True)
    #     return result.strip()
    
    1

        # soft_emb = soft_promts.mean(dim=0)
        # soft_emb = F.normalize(soft_emb, dim=-1).unsqueeze(0)  # [1, D]

        # # 2. å¥å­embeddingï¼Œshape [N, D]
        # with torch.no_grad():
        #     caption_embs = self.sentence_bert.encode(
        #         candidates, convert_to_tensor=True, device=self.device)
        # caption_embs = F.normalize(caption_embs, dim=-1)  # [N, D]

        # # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        # sims = torch.matmul(caption_embs, soft_emb.t()).squeeze(-1)  # [N]

        # # 4. æ’åºå¹¶è¿”å›
        # sorted_idx = torch.argsort(sims, descending=True)
        # ranked_list = [(candidates[i], sims[i].item()) for i in sorted_idx]
        # best_caption = ranked_list[0][0]

        # return best_caption, ranked_list



    def on_fit_start(self):
        if self.stage == 2 :
            self.accuracy.to(self.device)

    def forward(self, frames):
        """å°†è°ƒç”¨åº•å±‚æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€‚"""
        # 1. è®¡ç®—é‡‡æ ·ç´¢å¼•
        
        # 2. æŒ‰ç…§dim=1é‡‡æ ·
        # indices = torch.linspace(0, 48 - 1, steps=32).long()
        video_tokens = self.video_encoder.encode_videoQformer_visual(frames)[-1].last_hidden_state
        # video_embeddings = F.normalize(self.video_encoder.vision_proj(video_tokens), dim=-1) #[B, T, C]
        video_embeddings = self.video_encoder.vision_proj(video_tokens) #[B, T, C]
        fused_embeddings, soft_prompt = self.model(frames, video_embeddings)     # [B, embed_dim]     
        return fused_embeddings, soft_prompt

    def eos_position(self, ids, eos_token_id):
        # ids: [B, T]
        # è¿”å›æ¯ä¸ªåºåˆ—ä¸­ç¬¬ä¸€ä¸ªeos tokençš„ä½ç½®ï¼Œæ²¡æœ‰åˆ™ä¸ºT
        pos = []
        for row in ids.tolist():
            if eos_token_id in row:
                pos.append(row.index(eos_token_id))
            else:
                pos.append(len(row))
        return torch.tensor(pos, device=ids.device)

    def bart_encode(self, text_list):
        # è¿”å›æ¯ä¸ªå¥å­çš„ mean pooling encoder embedding
        inputs = self.tokenizer(text_list, return_tensors="pt", padding=True, truncation=True, max_length=64)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        # with torch.no_grad():
        outputs = self.llm.model.encoder(**inputs)
        last_hidden = outputs.last_hidden_state  # [batch, seq, hidden]
        # å¯¹æ¯ä¸ªå¥å­åš mean pooling
        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size()).float()
        summed = torch.sum(last_hidden * mask, dim=1)
        counts = torch.clamp(mask.sum(dim=1), min=1e-9)
        mean_pooled = summed / counts  # [batch, hidden]
        return mean_pooled

    def training_step(self, batch, batch_idx):
        frames, captions, video_ids = batch
        fused_emd, soft_prompt = self(frames)  # åªç”¨å›¾åƒï¼Œæ–‡æœ¬ç­‰ä¸‹å¤„ç†

        noise_std = 0.01
        # æ˜¾å¼ detachï¼Œé¿å…å¤šæ¬¡ä½¿ç”¨é€ æˆ autograd graph å †ç§¯
        fused_feature_1 = fused_emd + torch.randn_like(fused_emd) * noise_std
        fused_feature_2 = fused_emd + torch.randn_like(fused_emd) * noise_std
        soft_prompt_1 = self.model.vis_proj(fused_feature_1)
        soft_prompt_2 = self.model.vis_proj(fused_feature_2)
        infor_loss = simclr_infonce_loss(soft_prompt_1, soft_prompt_2)
        infor_loss_val = infor_loss.item()


        # Step 3: Dynamic Caption Selection
        selected_captions = []

        for idx in range(len(frames)):
            caption = captions[idx]  # list of multiple captions
            caption_lengths = torch.tensor([len(cap.split()) for cap in caption], device=self.device)
            target_length = caption_lengths.float().mean().item()  # dynamically set to average length

            with torch.no_grad():
                caption_embeddings = self.bart_encode(caption)  # [num_captions, hidden_dim]

            video_embedding = soft_prompt[idx].mean(dim=0, keepdim=True)  # [1, hidden_dim]

            sim_scores = F.cosine_similarity(video_embedding, caption_embeddings, dim=-1)  # [num_captions]

            length_scores = torch.exp(-((caption_lengths - target_length)**2) / (2*(2**2)))

            total_scores = 0.7 * sim_scores + 0.5 * length_scores
            top_k_indices = total_scores.topk(min(5, len(captions))).indices
            selected_index = top_k_indices[torch.randint(len(top_k_indices), (1,)).item()].item()
            # selected_index = total_scores.argmax().item()
            selected_captions.append(caption[selected_index])


        """è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ­¥ã€‚æ ¹æ®é˜¶æ®µè®¡ç®—ä¸åŒçš„æŸå¤±ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚"""
        # ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—ç”Ÿæˆæ–‡æœ¬ä¸çœŸå®æ–‡æœ¬çš„äº¤å‰ç†µæŸå¤±
        torch.cuda.reset_peak_memory_stats()  # ğŸ’¡ é‡ç½®æ˜¾å­˜å³°å€¼ç»Ÿè®¡

        # 1. æ„é€  hard promptï¼šè‡ªç„¶è¯­è¨€æç¤º
    
        # 2. æ„é€  ground-truth captions
        caption_tokenized = self.tokenizer(selected_captions, padding="max_length", truncation=True, max_length=32, return_tensors="pt").to(self.device)
        
        caption_ids = caption_tokenized.input_ids.to(self.device)
        
        labels = caption_ids.clone()
        labels[labels == self.tokenizer.pad_token_id] = -100         # pad -> -100
        encoder_outputs = BaseModelOutput(last_hidden_state=soft_prompt)
        mlm_inputs = self.tokenizer(selected_captions, return_tensors='pt', padding=True, truncation=True, max_length=16).to(self.device)
        mlm_labels = mlm_inputs.input_ids.clone()

        probability_matrix = torch.full(mlm_labels.shape, 0.15).to(labels.device)
        masked_indices = torch.bernoulli(probability_matrix).bool()
        mlm_labels[~masked_indices] = -100
        mlm_inputs.input_ids[masked_indices] = self.tokenizer.mask_token_id

        mlm_outputs = self.llm(**mlm_inputs, labels=mlm_labels)
        mlm_loss = mlm_outputs.loss 
        mlm_loss_val = mlm_loss.item()
        
        
        # ä½¿ç”¨hidden stateç­–ç•¥
        outputs = self.llm(
            input_ids=caption_ids,
            labels=labels,
            encoder_outputs=encoder_outputs
        )

        # ä½¿ç”¨scheduled samplingåçš„loss
        caption_loss = outputs.loss  
        caption_loss_val = caption_loss.item()

        with torch.no_grad():
            caption_embeds = self.llm.model.encoder(input_ids=caption_ids).last_hidden_state.to(self.device)

        alignment_loss = self.alignment(F.normalize(soft_prompt.mean(dim=1), dim=-1), F.normalize(caption_embeds.mean(dim=1), dim=-1) )
        alignment_loss_val = alignment_loss.item()
        
        # Step 6: Reinforcement Learning Loss
        sampled_caps = self.generate_caption_from_cross_attn(soft_prompt, num_beams=4, repetition_penalty=2.0, length_penalty=0.9)
        cider_reward = self.compute_cider_reward(sampled_caps, selected_captions)
        rl_loss = -torch.mean(cider_reward)
        rl_loss_val = rl_loss.item()
        
        diversity_loss = soft_prompt_cosine_diversity_loss(soft_prompt)
        diversity_loss_val = diversity_loss.item()
        loss = caption_loss +   infor_loss  + 0.2 * rl_loss + 0.2 * diversity_loss
        total_loss_val = loss.item()
        
        # 5. è®¡ç®— loss

        # è®°å½•è®­ç»ƒæŸå¤±
        self.log("train/infor_loss", infor_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/rl_loss", rl_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/diversity_loss", diversity_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/mlm_loss", mlm_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/alignment_loss", alignment_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/caption_loss", caption_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/train_loss", total_loss_val, on_step=True, on_epoch=True, prog_bar=True)
        
        

        # 5. æ˜¾å­˜ç»Ÿè®¡æ—¥å¿—
        peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
        self.log("train/peak_mem_MB", peak_mem, prog_bar=True, on_epoch=True)
        # Step 5: æ˜¾å¼é‡Šæ”¾é˜²æ³„æ¼
        del outputs, soft_prompt, caption_ids, labels, caption_loss, infor_loss
        torch.cuda.empty_cache()
        
        
        return loss
   
   
    @torch.no_grad()
    def validation_step(self, batch, batch_idx):
        """éªŒè¯è¿‡ç¨‹ï¼šç”Ÿæˆæè¿°å¹¶æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡ã€‚"""
        frames_tensor, caption ,video_id  = batch
        
        # 1. ç”Ÿæˆ captionï¼ˆé¢„æµ‹ï¼‰
        fused_emd,soft_prompts = self(frames_tensor)
        
        # tem_res = log_per_prompt_statistics(soft_prompts)
        
        # pask key val ç­–ç•¥ 1
        generated_captions = self.generate_caption_from_cross_attn(
            soft_prompt=soft_prompts
        )
        
        generated_captions = [strip_prefix(p) for p in generated_captions]
        generated_captions = [polish_caption_for_rouge(cap) for cap in generated_captions]
        # 2. æ›´æ–° NLP-metricsï¼ˆBLEU, METEOR, CIDEr ç­‰ï¼‰
        self.accuracy.update(generated_captions, caption)
        
        if batch_idx == 0 and self.trainer.is_global_zero:
            # for i, (pred, ref) in enumerate(zip(generated_captions, caption)):
            #     if i >= 2:  # æœ€å¤šè®°å½• 2 ä¸ªæ ·æœ¬ï¼Œé˜²æ­¢æ—¥å¿—è¿‡å¤š
            #         break
            #     self.logger.experiment.add_text(
            #         tag=f"val_caption_epoch{self.current_epoch}_sample{i}",
            #         text_string=f"**GT:** {ref[:10]}\n**Pred:** {pred}",
            #         global_step=self.global_step
            #     )

            # éšæœºé€‰2ä¸ªæ ·æœ¬
            idxs = random.sample(range(len(generated_captions)), min(2, len(generated_captions)))
            for j, i in enumerate(idxs):
                pred = generated_captions[i]
                ref = caption[i]
                vid = video_id[i]
                self.logger.experiment.add_text(
                    tag=f"val_caption_epoch{self.current_epoch}_sample{j}_vid{vid}",
                    text_string=f"**video_id:** {vid}\n**GT:** {ref[:10]}\n**Pred:** {pred}",
                    global_step=self.global_step
                )
        
        del fused_emd, soft_prompts, generated_captions
        torch.cuda.empty_cache()
    
    @torch.no_grad()
    def on_validation_epoch_end(self):
        score_dict = self.accuracy.compute()  # self.val æ˜¯éªŒè¯é›†çš„ reference caption æ•°æ®

        # åˆ†åˆ«è®°å½•å„é¡¹æŒ‡æ ‡
        self.log('val/harmonic_mean_bleu_meteor', self.accuracy.harmonice, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu1', self.accuracy.bleu1, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu2', self.accuracy.bleu2, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu3', self.accuracy.bleu3, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/bleu4', self.accuracy.bleu4, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/meteor', self.accuracy.meteor, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/cider', self.accuracy.cider, on_epoch=True, prog_bar=True, logger=True)
        self.log('val/rougel', self.accuracy.rougel, on_epoch=True, prog_bar=True, logger=True)
        peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
        self.log("val/peak_mem_MB", peak_mem, prog_bar=True, on_epoch=True)

        # é‡ç½®æŒ‡æ ‡ç¼“å­˜
        self.accuracy.reset()

    
    def test_step(self, batch, batch_idx):
        cider_metric = self.accuracy.nlp_metric_cider

        """æµ‹è¯•è¿‡ç¨‹ï¼šç”Ÿæˆæè¿°å¹¶æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ï¼Œç”¨äºè®¡ç®—æŒ‡æ ‡ã€‚"""
        frames_tensor, caption, video_ids = batch
        # 1. ç”Ÿæˆ captionï¼ˆé¢„æµ‹ï¼‰
        fused_emd,soft_prompts = self(frames_tensor)        
        generated_captions = self.generate_caption_from_cross_attn(
            soft_prompt=soft_prompts
        )
        
        generated_captions = [strip_prefix(p) for p in generated_captions]
        generated_captions = [polish_caption_for_rouge(cap) for cap in generated_captions]
        # self.update_json('/workspace/YOLO-World/scripts/yoloworld_lightning/data/msrvtt/soft_prompt',video_ids, soft_prompts)
        output_dir = '/workspace/YOLO-World/scripts/yoloworld_lightning/data/msrvtt/txt'
        os.makedirs(output_dir, exist_ok=True)

        for vid, cap in zip(video_ids, generated_captions):
            txt_path = os.path.join(output_dir, f"{vid}.txt")
            # å¦‚æœæƒ³æ¯ä¸ªæ–‡ä»¶åªä¿å­˜ä¸€æ¡captionï¼š
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(cap.strip() + "\n")

        # _, baseline_cider_scores = cider_metric(generated_captions, caption)
        # for i, c in enumerate(baseline_cider_scores):
        #     if c < 0.55:
        #         self.video_ids.append(i)
                
        # 2. æ›´æ–° NLP-metricsï¼ˆBLEU, METEOR, CIDEr ç­‰ï¼‰
        self.accuracy.update(generated_captions, caption)
        
        if batch_idx == 0 and self.trainer.is_global_zero:
            for i, (pred, ref) in enumerate(zip(generated_captions, caption)):
                if i >= 2:  # æœ€å¤šè®°å½• 2 ä¸ªæ ·æœ¬ï¼Œé˜²æ­¢æ—¥å¿—è¿‡å¤š
                    break
                self.logger.experiment.add_text(
                    tag=f"val_caption_epoch{self.current_epoch}_sample{i}",
                    text_string=f"**GT:** {ref}\n**Pred:** {pred}",
                    global_step=self.global_step
                )
        
        del fused_emd, soft_prompts, generated_captions
        torch.cuda.empty_cache()
    
    
    def on_test_epoch_end(self):
        # print(self.video_ids)
        # 1. è®¡ç®—æ‰€æœ‰ NLP æŒ‡æ ‡
        score_dict = self.accuracy.compute()

        # 2. è®°å½•æ¯ä¸ªæŒ‡æ ‡
        self.log('test/harmonic_mean_bleu_meteor', torch.tensor(self.accuracy.harmonice,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu1', torch.tensor(self.accuracy.bleu1,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu2', torch.tensor(self.accuracy.bleu2,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu3', torch.tensor(self.accuracy.bleu3,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/bleu4', torch.tensor(self.accuracy.bleu4,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/meteor', torch.tensor(self.accuracy.meteor,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/cider', torch.tensor(self.accuracy.cider,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)
        self.log('test/rougel', torch.tensor(self.accuracy.rougel,device=self.device), on_epoch=True, prog_bar=True, logger=True,sync_dist=True)

        # å¯é€‰ï¼šè®°å½•æ±‡æ€»æŒ‡æ ‡
        # self.log("test/harmonic_mean_bleu_meteor", score_dict["harmonic_mean"], prog_bar=True, logger=True)

        # 3. æ¸…ç©ºç¼“å­˜
        self.accuracy.reset()


    def update_json(self, output_dir, video_ids, soft_prompt):
        os.makedirs(output_dir, exist_ok=True)

        for vid, caption in zip(video_ids, soft_prompt):
            # ä¿è¯ vid æ˜¯å­—ç¬¦ä¸²
            if not isinstance(vid, str):
                vid = str(vid)
            # æ„é€ è·¯å¾„
            out_path = os.path.join(output_dir, f"{vid}.pt")
            # å†™å…¥æ–‡ä»¶
            torch.save(caption, out_path)

    
            
    def configure_optimizers(self):
        """æ ¹æ®é˜¶æ®µè¿”å›ç›¸åº”çš„ä¼˜åŒ–å™¨ï¼ˆå’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰ã€‚"""
            
        # 1. æ”¶é›†å„ç»„å‚æ•°
        vis_proj_params     = list(self.model.vis_proj.parameters())
        fusion_params = list( self.model.fusion_module.parameters())
        text_enhancer_params = list(self.model.detector.neck.text_enhancer.parameters())
        projoctor_params = list(self.model.projector.parameters())
        word_embedding_params = list(self.llm.model.decoder.embed_tokens.parameters())
        position_embedding_params = list(self.llm.model.decoder.embed_positions.parameters())

        # qformer_tokens_params = [self.video_encoder.query_tokens]
        decoder_layernorm = list(self.llm.model.decoder.layernorm_embedding.parameters())
        # 2. è§£å†» LLM çš„ decoder æœ€å2å±‚
        dec_params = []
        total_dec_layers = self.llm.config.decoder_layers
        dec_target_ids = {total_dec_layers - i - 1 for i in range( self.unfreeze_llm_last_n )}
        for name, param in self.llm.model.decoder.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                dec_params.append(param)
        enc_params = []
        total_enc_layers = self.llm.config.encoder_layers
        enc_target_ids = {total_enc_layers - i - 1 for i in range(self.unfreze_llm_encoder_n)}
        for name, param in self.llm.model.encoder.named_parameters():
            if any(f"layers.{i}." in name for i in enc_target_ids):
                enc_params.append(param)
                
        video_encoder_params = []
        num_layers = len(self.video_encoder.video_Qformer.bert.encoder.layer)
        dec_target_ids = {num_layers - i - 1 for i in range(self.unfreeze_mae_n)}
        for name, param in self.video_encoder.video_Qformer.bert.encoder.layer.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                video_encoder_params.append(param)
                
        video_encoder_qformer_params = []
        num_layers = len(self.video_encoder.Qformer.bert.encoder.layer)
        dec_target_ids = {num_layers - i - 1 for i in range(self.unfreeze_qformer)}
        for name, param in self.video_encoder.Qformer.bert.encoder.layer.named_parameters():
            if any(f"layers.{i}." in name for i in dec_target_ids):
                video_encoder_qformer_params.append(param)
               
        total_steps = int(self.trainer.estimated_stepping_batches)
        # image_pooler_params = list(self.model.detector.neck.downsample_layers.parameters())
        # cross_enhancer_params = list(self.model.detector.neck.bottom_up_layers.parameters())
        # top_down_layer_params = list(self.model.detector.neck.top_down_layers.parameters())
        
         
        # 3. å®šä¹‰ä¼˜åŒ–å™¨ï¼Œåˆ†åˆ«ç»™ä¸åŒç»„ä¸åŒ lr
        optimizer = torch.optim.AdamW([
            {"params": vis_proj_params , "lr": 9e-6 , "weight_decay": 0.01},
            {"params": projoctor_params, "lr": 9e-6, "weight_decay": 0.01},
            {"params": fusion_params  , "lr": 9e-6, "weight_decay": 0.01},
            {"params": text_enhancer_params , "lr": 9e-6, "weight_decay": 0.01},
            {"params": dec_params , "lr": 9e-6, "weight_decay": 0.01},
            {"params":  word_embedding_params, "lr": 9e-6, "weight_decay": 0.01},
            {"params": decoder_layernorm, "lr": 9e-6, "weight_decay": 0.01},
            {"params": position_embedding_params, "lr": 9e-6, "weight_decay": 0.01},
            
            {"params": enc_params, "lr": 9e-6, "weight_decay": 0.01},
            {"params": video_encoder_params, "lr": 9e-6, "weight_decay": 0.01},  
            {"params": video_encoder_qformer_params, "lr": 9e-6, "weight_decay": 0.01},
        ])

        # 4. å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆcosine+warmup ä¸¾ä¾‹ï¼‰
        scheduler = get_cosine_schedule_with_warmup(
                optimizer,
                num_warmup_steps=int(0.3* total_steps),  # å‰10%æ­¥æ•°ç”¨äºwarmup
                num_training_steps=int( total_steps),
            )       
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",   # æ¯stepæ›´æ–°
                "frequency": 1,
            },
            "monitor": "val_cider",  # è¿™è¡Œå…¶å®å¯ä»¥ä¸åŠ ï¼Œå› ä¸ºæˆ‘ä»¬ç”¨cosine
        }
               
# def polish_english(sentence, tokenizer, model, max_new_tokens=40):
#     # æŒ‡ä»¤promptï¼Œä¸“ä¸ºè‹±æ–‡é”™å¥æ¶¦è‰²
#     prompt = (
#         "Please correct and rewrite the following sentence in fluent, grammatically correct English:\n"
#         f"{sentence}\n"
#         "Corrected:"
#     )
#     inputs = tokenizer(prompt, return_tensors="pt")
#     outputs = model.generate(
#         **inputs,
#         max_new_tokens=max_new_tokens,
#         do_sample=False,
#         eos_token_id=tokenizer.eos_token_id,
#         pad_token_id=tokenizer.eos_token_id,
#         use_cache=False
#     )
#     result = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     # å¯åšåå¤„ç†åªæå–Corrected:åé¢çš„å†…å®¹
#     if "Corrected:" in result:
#         result = result.split("Corrected:")[-1].strip()
#     return result